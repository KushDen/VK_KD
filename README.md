# VK_KD
Example of knowledge distillation

**Введение**

*Смотрите ноутбук Imagewoof_KD.ipynb в первую очередь!!! Там я постарался все подробно расписать. В другом ноутбуке аналогичные моменты опускал*

**Imagewoof_KD.ipynb**

В ноутбуке Imagewoof_KD я провожу 2 эксперимента. Набор данных - датасет Imagewoof. Он считается усложненным датасетом для задачи классификации объектов на изображении. То есть есть картинка - на ней объект - модель должна классифицировать этот объект.

В первом эксперименте я разрабатываю (конструирую архитектуру, подбираю гиперпараметры) относительно небольшую модель для классификации и обучаю ее. Запоминаю качество распознавания полученной модели.

Также в первом эксперименте я формирую еще более маленькую известную модель Le-Net5 и также провожу обучение на нашем датасете.

То есть я исследую две маленькие модели - свою и Le-Net5. И для каждой провожу эксперименты отдельно.

Во втором эксперименте я загружаю большую модель resnet50 с предобученными весами и дообучаю эту модель на нашем датасете. Называем эту модель - большой моделью, на которую как раз будет опираться маленькая модель - distilled model.

Маленькая модель состоит из тех же слоев и гиперпараметров как и модель из первого эксперимента (в первом эксперименте две модели, обучение совместно с большой моделью производится как для мною сконструированной модели так и для модели Le-Net5). Но теперь при обучении мы будем использовать не только реальные правильные ответы, но и ответы soft target - это более сглаженное распределедение вероятностей хорошо обученной большой модели. То есть для формирования вероятностного распределения большой модели для классов классификации используется не обычный совтмакс, а софтмакс с повышенной температурой, для обеспечения мягкости распределения.

То есть теперь, маленькая модель тренируется таким образом, чтобы конечные распределения вероятностей distilled model и большой модели были максимально близки. Это обеспечивается как раз KL дивергенцией - образно говоря мера расстояния между распределениями - ее мы устремляем к нулю, то есть эта KL дивергенция может играть роль функции потерь, которую мы минимизируем. Но также мы минимизируем вторую функцию потерь - обычную кроссэнтропию между выходом маленькой модели и реальными правильными ответами. Эти две функции складываются с определенными коэффицентами и общая получившаяся функция минимизируется.

Предполагается, что маленькая модель (distilled model) сможет изъять и получить больше информации от большой модели о нашей выборке, чем если просто обучать маленькую модель без помощи большой. 

То есть в конечном итоге хотелось бы получить результаты распознавания маленькой модели во втором эксперименте выше, чем распознавание этой же модели из первого эксперимента.

В самом коде я дополнительно привожу подробное объяснение. В конце ноутбука приведен отчет.

**Imagenette_KD.ipynb**

В этом ноутбуке я провожу такие же эксперименты, но с датасетом Imagenette. Отчет также приведен в конце.

**Инструкция запуска**

Все просто, в ноутбуках есть пометка "Эксперимент 1", там поочередно нужно запускать все ячейки. Есть "Эксперимент 2" - там аналгично нужно поочередно запускать все ячейки. В этих экмпериментах производится закачка данных, разработка и обучение моделей. Эксперименты независимы по ячейкам, то есть можно запустить второй эксперимент не запуская изначально первый и наоборот. Все эксперименты проводились на площадке google colab для ускорения процесса обучения. 
